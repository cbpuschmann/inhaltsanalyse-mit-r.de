---
title: "Automatisierte Inhaltsanalyse mit R"
author: "Cornelius Puschmann"
subtitle: Datenimport
output: html_notebook
---

<!---
Todos
* ...
* ...
* ...
-->

Der Import von Daten gehört nach meiner Erfahrung zu den wichtigsten Themen, wenn es um studentische Projekte unter Nutzung der automatisierten Inhaltsanalyse geht. Aber auch bei großen Forschungvorhaben sind die Verfügbarkeit, Vorverarbeitung und Speicherung von Textdaten grundsätzlich bedeutende Aspekte eines Projekts, die sorgfältig geplant werden sollten. Daher mag es verwundern, dass ein Kapitel, welches Werkzeuge und Techniken für den Import von Textinhalten nach R vorstellt, nicht gleich zu Beginn dieser Einführung steht, sondern erst ganz am Schluss. Grund für diesen Umstand ist die vielfache Erfahrung, dass die erfolgreiche Datenbeschaffung und der Import von Daten oft (zu) viel Zeit in Anspruch nehmen, worunter die Analyse spürbar leidet. Gerade in der Lehre ist dies aus offensichtlichen Gründen ein Problem –– nicht die intellektuell eher langweilige aber immer wichtige Erhebung, Speicherung und Vorverarbeitung von Texten sollte im Mittelpunkt eines solchen Überblicks stehen, sondern die Bearbeitung sozialwissenschaftlicher Fragestellungen. Aus diesem Grund gebe ich in dieser Einführung bewusst bereits bereinigte Datensätze vor, auch wenn dies gewisse Einschränkungen bezüglich der Vielfalt mit sich bringt.

Ein weiterer Gesichtspunkt, der bei der Erstellung neuer Korpora mitunter vergessen wird, liegt in der stetig wachsenden Zahl exzellenter Datensätze auf Plattformen wie [GESIS Datorium](https://datorium.gesis.org/), dem [Harvard Dataverse](https://dataverse.harvard.edu/) und [Zenodo](https://zenodo.org/) begründet, die man häufig nachnutzen kann, und die sich über Werkzeuge wie die [Google Datensatzsuche](https://toolbox.google.com/datasetsearch) oder [Mendeley Data](https://data.mendeley.com) leicht auffinden lassen. Das ist auch dann interessant, wenn Rohdaten zwar theoretisch abrufbar sind, aber erst aufwändig vorverarbeitet werden müssen, bevor sich aus ihnen ein Korpus mit gut strukturierten Metadaten erstellen lässt (ein Beispiel hierfür sind etwa Dokumente öffentlicher Institutionen im PDF-Format). Korpora wie das hier verwendete Bundestagskorpus auf Grundlage von offenesparlament.de lassen sich nur mit ausgesprochen hohem Aufwand selbst aus den Originaldokumenten in einen geordneten Data Frame überführen. Da ist es intelligenter, standardisierte und gut dokumentierte Ressourcen zu verwenden (sofern diese vorliegen), als das sprichwörtliche Rad neu zu erfinden. Muss man doch selbst bereinigen und anreichern, sollte man dann aber auch unbedingt über eine Veröffentlichung der Daten auf einer der genannten Plattformen nachdenken -- gemäß den Prinzipien von Open Data und dem Motto '[pay it forward](https://en.wikipedia.org/wiki/Pay_it_forward)'. 

Natürlich gibt es sehr viele Vorhaben, für die man eigens Daten erheben muss –– von Presseberichterstattung bis hin zu sozialen Medien. Folgend gebe ich eine Überblick über Ansätze, durch die dies möglich wird. Einerseits handelt es sich hierbei um handgestrickte Skripte, die exportierte Daten aus Diensten wie Lexis Nexis oder von Webseiten einlesen (in einigen Fällen handelt es sich auch um Pakete, die genau dies vereinfachen sollen), andererseits um Verfahren, die durch Programmierschnittstellen (APIs) realisiert werden und entsprechende R–Bibliotheken (Rfacebook, rtweet) nutzen. Fehler sind zwar grundsätzlich immer möglich, im ersten Modell schleichen sie sich allerdings besonders leicht ein, weil die exportierten Dateien oft Fehler oder Idiosynkratien aufweisen, die beim Einlesen nicht unbedingt sofort offenkundig sind. Dafür muss man den (kommerziellen) APIs, die man für die Erhebung verwendet, bezüglich der Datenqualität vertrauen, was nicht unbedingt vorbehaltlos geschehen sollte. 

Für die folgenden Dienste stelle ich in diesem Abschnitt Importfunktionen und -strategien vor:

* Twitter
* Facebook
* YouTube
* Reddit
* Wikipedia
* Web
* LexisNexis
* Factiva
* COSMAS
* MediaCloud
* arXiv
* Web of Science

Wie schnell ersichtlich wird, handelt es sich um Social Media–APIs, Nachrichtendatenbanken sowie Dienste, die wissenschaftliche Texte vorhalten. Es fehlen u.a. Parlamentskorpora und andere öffentliche Texte, was damit zusammenhängt, dass diese über Angebote Google Dataset Search relaltiv gut gefunden werden können, sofern sie maschinenlesbar abgelegt sind.

Die Ansätze sind wie beschrieben sehr heterogen: von eigenes entwickeltem Code für das Einlesen von Textdaten (COSMAS, Web of Science) über Module existierender R-Bibliotheken (Lexis Nexis, Factiva), welche diese Aufgabe übernehmen, bis hin zu Daten-APIs, die über entsprechende Bibliotheken bereits strukturierte Daten liefern (arXiv, MediaCloud, Twitter, Facebook, YouTube). 

Auch hier beginnen wir damit, die notwendigen Bibliotheken zu installieren bzw. zu laden.

```{r Installation und Laden der benötigten R-Bibliotheken, message = FALSE}
if(!require("quanteda")) {install.packages("quanteda"); library("quanteda")}
if(!require("tidyverse")) {install.packages("tidyverse"); library("tidyverse")}
if(!require("tm")) {install.packages("tm"); library("tm")}
if(!require("tm.plugin.lexisnexis")) {install.packages("tm.plugin.lexisnexis"); library("tm.plugin.lexisnexis")}
if(!require("tm.plugin.factiva")) {install.packages("tm.plugin.factiva"); library("tm.plugin.factiva")}
if(!require("aRxiv")) {install.packages("aRxiv"); library("aRxiv")}
theme_set(theme_minimal())
```


# Soziale Medien

## Twitter

Der Kurznachrichtendienst Twitter ist zweifelsfrei eine der populärsten Social Media-Datenquellen in der sozialwissenschaftlichen Forschung. Sprichwörtliche Berge von Studien türmen sich auf, wenn man in einer einschlägigen wissenschaftlichen Suchmaschine wie etwa Google Scholar oder Microsoft Academic [nach Beiträgen zu Twitter sucht](https://academic.microsoft.com/search?q=twitter). Kaum ein Thema, von der Vorhersage von Wahlergebnissen hin zum Katastrophenschutz wird nich anhand von Twitter-Daten untersucht. Dies ist umso erstaunlicher, als dass Twitter im Vergleich zu anderen Plattformen eher klein und in Bezug auf die Repräsentativität seiner Nutzer eingeschränkte nützliche Plattform im Vergleich zu Facebook oder YouTube ist. Was Twitter jedoch in besonderem Maße kennzeichnet ist die Offenheit seiner Daten. Während andere Social Media-Plattformen zunehmend restriktiv agieren, sind Daten aus Twitter weitgehend frei verfügbar. Daten aus der Twitter API können vergleichsweise bequem mittels zahlreicher nutzerfreundlicher Bibliotheken für Python und R extrahiert und ausgewertet werden.

Für R ist in besonderem Maße das Paket [rtweet](https://rtweet.info) relevant, mit dem sich besonders leicht Twitter–Daten extrahieren lassen.


## Facebook und Instragram

Mit rund XXX Mio Nutzern ist Facebook die größte Social Networking Site weltweit. Wie auch Twitter besitzt Facebook eine API, über die sich Daten aus der Plattform abrufen lassen, allerdings hat sich hier als Reaktion auf den Cambridge Analytica–Skandal die Lage mit Blick auf diesen Datenzugang dramatisch verändert. Konnte mal noch 2015 problemlos Postings und Kommentare aus öffentlichen Facebookseiten mit  Libraries wie etwa [Rfacebook](https://cran.r-project.org/package=Rfacebook) abrufen, ist dies heute nur noch in absoluten Ausnahmefällen möglich.

Ein Zugangsweg, den wir auch im Zusammenhang mit Instagram vorstellen bleibt allerdings: die gleichnamige  API des Anbieters CrowdTangle der seit XXX zu Facebook gehört. Über die CrowdTangle API können einerseits URLs und andererseits öffentliche Facebook Posts abgerufen werden, allerdings mit einigen Beschränkungen gegenüber den Optionen, welche die Facebook Graph API noch bis Mitte 2019 für existierende Projekte geboten hat. Seit diesem Zeitpunkt ist ein gezieltes Abrufen von Daten nur noch mittels Scraping möglich.

Die CrowdTangle API bietet immerhin Zugriff auf umfangreichere Metriken 
hur Messung ....


## YouTube 

Im Vergleich zu Twitter und Facebook kann man bei YouTube in Bezug auf die kommunikationswissenschaftliche Forschung davon sprechen, dass die Plattform in Relation zu ihrer Größe und Bedeutung insgesamt stark unterschätzt wird, bzw. unterforscht ist. Natürlich nutzen Sozialwissenschaftler auch Daten aus YouTube, aber das geschieht in geringerem Umfang als bei Twitter oder Facebook, und als man das gemeinhin erwarten würde.


# Journalistische Medien

## LexisNexis

LexisNexis ist ein kommerzieller Anbieter von Recherchelösungen für die Volltextsuche in internationalen Periodika, Presse- und Wirtschaftsinformationen. Einen Schwerpunkt stellen Rechtsinformationen dar, es existieren aber auch noch zahlreiche weitere Angebote, die zum Teil von Universitätsbibliotheken lizensiert werden und dementsprechen über das Uninetz abrufbar sind (oder über VPN auch außerhalb). Durch [Nexis](http://www.nexis.com/) -- strengenommen ist LexisNexis der Name des Anbieters -- lassen sich internationale Pressevolltexte rechervieren und anschließend in unterschiedlichen Formaten exportieren. Diese Daten (in der Regel in den Formaten PDF, RTF, HTML oder TXT) lassen sich leicht in R einlesen, allerdings gibt es keine einfache Methode, den eigentlichen Textinhalt von Metadaten wie dem Medium, dem Publikationsdatum, dem Autor usw. zu trennen. 

Im ersten Schritt lesen wir mittels [LexisNexisSource()](https://www.rdocumentation.org/packages/tm.plugin.lexisnexis/versions/1.4.0/topics/LexisNexisSource) die Rohdaten ein, im zweiten erstellen wir mittels [Corpus()](https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/Corpus) ein Korpus aus den Texten. Diese Funktionen kommen nicht aus quanteda oder dem Standardfunktionsumfang von R, sondern sind Teile des Pakets [tm](https://cran.r-project.org/package=tm) bzw der Erweiterung [tm.plugin.lexisnexis](https://cran.r-project.org/package=tm.plugin.lexisnexis).

```{r Daten importieren und parsen (mittels tm und tm.plugin.lexisnexis)}
lexisnexis <- LexisNexisSource("daten/lexisnexis/Nachrichten2018-03-05_13-35.HTML")
tm.lexisnexis <- Corpus(lexisnexis, readerControl = list(language = NA))
```

Nun übergeben wir das in tm erstellte Korpus an quanteda -- glücklicherweise aktzeptiert die quanteda-Funktion [corpus](https://quanteda.io/reference/corpus.html) neben anderen Formaten auch widerspruchslos Korpora, die mit tm generiert wurden. 

```{r Daten in quanteda-Korpus umwandeln}
korpus.lexisnexis <- corpus(tm.lexisnexis)
korpus.lexisnexis.stats <- summary(korpus.lexisnexis, n = 1000000)
korpus.lexisnexis
head(korpus.lexisnexis.stats)
```

Wie man sieht, funktioniert der Import insgesamgt gut, von einigen Metadatenfeldern einmal abgesehen. Konkret wird das Datum nicht richtig erkannt, was mit den Einstellungen der Funktion LexisNexisSource zu tun hat, die nun englische und französiche Daten (d.h. Wochentage und Monatsnamen) versteht. Das lässt sich allerdings leicht korregieren, wenn man etwas kreativ wird. 


## Factiva

Factiva ist ein äquivalentes Angebot zu Nexis, Anbieter ist hier Dow Jones. Ähnlich wie bei Nexis liegt hier ein relativ starker Fokus auf Wirtschrafts- und Finanzpresse, wobei die internationale Abdeckung beider Anbieter sich nicht sehr markant unterscheidet. Es spricht auch nichts dagegen, Daten aus beiden Quellen zusammenzufassen, um eine möglichst gute Abdeckung zu erreichen. 

Das Vorgehen bei Factiva ist exakt das selbe wie bei Nexis, mit dem einzigen Unterschied, dass die Funktion [FactivaSource]() zum Einsatz kommt. 


```{r Factiva-Daten importieren und parsen}
#factiva <- FactivaSource("daten/factiva/Factiva.html", format = "HTML")
factiva <- FactivaSource("daten/factiva/Factiva01.htm", format = "HTML")
tm.factiva <- Corpus(factiva, readerControl = list(language = NA))
```

Wieder übergeben wir das in tm erstellte Korpus an quanteda, mit dem Ergebnis, dass wir ein relativ sauberes quanteda-Korpus erhalten mit dem wir dann wie gewohnt weiterarbeiten können. 

```{r Factiva-Daten in quanteda-Korpus umwandeln und speichern}
korpus.factiva <- corpus(tm.factiva)
korpus.factiva.stats <- summary(korpus.factiva, n = 1000000)
korpus.factiva
head(korpus.factiva.stats)
#save(korpus.factiva, korpus.factiva.stats, file = "factiva.korpus.RData")
```


## COSMAS

Nexis und in geringerem Umfang auch Factiva werden in der Kommunikations- und Medienwissenschaft, aber auch in anderen Fachbereichen, umfassend genutzt, um große Datensätze für quantitative Inhaltsanalysen aufzubauen. Bei [COSMAS](https://www.ids-mannheim.de/cosmas2/), einer Korpus-Datenbank des [Leibniz-Instituts für Deutsche Sprache (IDS)](http://www1.ids-mannheim.de/) handelt es sich um einen wissenschaftlichen Dienst eines öffentlichen Forschungsinstituts, der in vielerlei Hinsicht Nexis und Factiva überlegen ist, und der zugleich in den Sozialwissenschaften derzeit noch kaum Anwendung findet.

Ähnlich wie auch bei der zuvor beschriebenen Diensten werden COSMAS-Daten über eine Webschnittstelle heruntergeladen, und anschließend mit R eingelesen ("geparsed"). Das Korpus-Tool ist [hier](https://cosmas2.ids-mannheim.de/cosmas2-web/) erreichbar, zuvor muss man sich [hier](https://www.ids-mannheim.de/cosmas2/projekt/registrierung/) kostenlos registrieren (für wissenschaftliche Einrichtungen). Die [Hilfe](http://www.ids-mannheim.de/cosmas2/web-app/hilfe/allgemein/) zum COSMAS-Interface ist zwar ein wenig altmodisch (wie auch das Design des Dienstes selbst), gibt aber über alle wichtigen Aspekte wie etwa Korpusauswahl, Recherche, das Anzeigen von Resultaten und den Export von Daten informativ Aufschluss. Zu beachten ist in diesem Zusammenhang, das COSMAS nur längere Passagen extrahiert, nicht aber den vollständigen Artikeltext. Dies mag zunächst wie ein entscheidender Nachteil gegenüber anderen Ansätzen wirken, allerdings ist für die hier vorstellten Verfahren oftmals nicht entscheidend, dass wirklich der gesamte Artikeltext vorliegt. Solange der Kontext (bspw. mehrere Absätze vor und nach dem Suchbegriff) vorliegt, sind die Auswirkungen i.d.R. marginal. 

Der folgende Code-Abschnitt lädt aus einem eigens für diese Einführung erstellten Zusatzskript die Funktion *import_cosmas()*, welche die Argumente *path* und *pattern* annimt (vergleiche [dir()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/list.files.html)). Die Funktion liest die aus COSMAS exportierten Textdaten ein und wandelt sie in einen Data Frame um, aus dem sich dann anschließend ein Korpus mit umfassenden Metadaten erstellen lässt. 

```{r COSMAS-Daten importieren und parsen}
source("verschiedenes/inhaltsanalyse_import_cosmas.R", local = T)
daten.cosmas <- import_cosmas(path = "daten/cosmas/finanzkrise", pattern = "finanzkrise_.*.TXT", verbose = F)
```

Die Korpuserstellung funktioniert ähnlich wie bei Nexis und Factiva. Allerdings ist der Input hier ein Data Frame, daher müssen wir explizit machem, welche Variable die Dokument-ID und welche den Text enthält. Alle weiteren Spalten des Data Frames enthalten implizit docvars. 

```{r COSMAS-Daten in quanteda-Korpus umwandeln und speichern}
korpus.cosmas <- corpus(daten.cosmas, docid_field = "id", text_field = "text") 
korpus.cosmas.stats <- summary(korpus.cosmas, n = 1000000)
head(korpus.cosmas.stats)
#save(korpus.cosmas, korpus.cosmas.stats, file = "cosmas.korpus.RData")
```


## MediaCloud

xxx


## GDELT

xxx


# Wissenschaftliche Texte 

## arXiv

Nich nur Pressetexte lassen sich mit R abrufen, einlesen und in Korpora überführen. Auch wissenschaftliche Quellen bieten sich für die Auswertung an, um etwa Fragen zur Entwicklung von Forschungsfeldern, zur Interdisziplinarität oder zu Entwicklung von Wissenschaftsbiographien anhand von Inhaltsdaten zu bearbeiten. 

Das [arXiv](https://arxiv.org/) (gesprochen wie "archive") ist das weltweite größte [Pre-Print-Repositorium](https://de.wikipedia.org/wiki/Dokumentenserver), mit mehr als 1,5 Mio. Arbeitspapieren, vor allem aus den Bereichen Physik, Mathematik und Informatik. Da alle Papiere Open Access-Veröffentlichungen sind, lassen sich über die arXiv-API nahezu unbeschränkt Daten ausdem Repositorium herunterladen, wobei man darauf achten sollte, die kostenlos angebotenen Serverressourcen nicht zu überlasten, indem man in kurzer Folge zigtausende Anfragen stellt.

Das R-Paket [aRxiv](https://github.com/ropensci/aRxiv), welches Teil der großartigen Initiative  [ropensci](https://ropensci.org/) ist, innerhalb derer Pakete zusammengestellt werden, die besonders für (natur)wissenschaftliche Forschung interessant sind, erleichtert den Zugriff auf Daten aus arXiv ungemein, wie das folgenden Beispiel belegt. Darin stellen wir einen Korpus aus Abstracts zusammen, welche den Begriff 'social media' enthalten. 

```{r Daten aus arXiv importieren}
daten.arxiv <- arxiv_search('abs:"social media"', limit = 2000)
korpus.arxiv <- corpus(daten.arxiv, docid_field = "id", text_field = "abstract")
korpus.arxiv.stats <- summary(korpus.arxiv, n = 1000000)
head(korpus.arxiv.stats)
```

Details dazu, wie Anfragen konstruiert werden, findet man im [Handbuch zum R-Paket aRxiv](https://cran.r-project.org/package=aRxiv) sowie im der [Dokumentation der arXiv API](https://arxiv.org/help/api/user-manual#query_details).


## Web of Science

xxx


