---
title: "Automatisierte Inhaltsanalyse mit R"
author: "Cornelius Puschmann"
subtitle: Überwachtes Maschinelles Lernen (RTT legacy code)
output: html_notebook
---

### Klassifikation der New York Times-Daten mit RTextTools

In einem nächsten Schritt klassifizieren wir nun die Daten erneut, allerdings mit vier unterschiedlichen Algorithmen, welche im Paket [RTextTools](https://cran.r-project.org/package=RTextTools) enthalten sind. 

Dazu wird zunächst der sog. Container vorbereitet, welcher Textdaten und Labels enthält. Zudem wird festgelegt, welcher Teil des Datensatzes für das Training verwendet und welcher Teil für die eigentliche Klassifikation herangezogen wird (man spricht auch von Trainings- und Testdaten). Es ist üblich, circa 10% der Daten für Testzwecke zurückzuhalten.

```{r Container für die Klassifikation mit RTextTools vorbereiten}
container <- create_container(convert(meine.dfm.trim, to = "matrix"), korpus.nyt.stats$Topic_2digit, trainSize = 1:27775, testSize = 27776:30862, virgin = FALSE)
```

Nun werden vier verschiedene Modelle trainiert. Im Unterschied zu unserem Naive Bayes-Klassifikator dauert dies deutlich länger, was zu einen mit der mathematschen Komplexität des jeweiligen Verfahrens zu tun hat, und zum anderen damit zusammenhängt, wie der jeweilige Algorithmus in R implementiert ist (die NB-Implementation von quanteda ist insgesamt sehr viel effizienter als RTextTools, welches sich seinerseits in Teilen auf sehr viel ältere und langsamere Pakete verlässt). 

Wem der Trainingsprozess zu lange dauert, der kann auch einfach vier bereits trainierte Modelle laden. 

```{r RTextTools-Modelle trainieren}
load("daten/nytimes/nyt.modelle.RData")

# primäre Modelle
#modell.SVM <- train_model(container,"SVM")
#modell.GLMNET <- train_model(container,"GLMNET")
#modell.MAXENT <- train_model(container,"MAXENT")
#modell.SLDA <- train_model(container,"SLDA")

# weitere Modelle, funktionieren z.T. nicht
#modell.BOOSTING <- train_model(container,"BOOSTING")
#modell.BAGGING <- train_model(container,"BAGGING")
#modell.RF <- train_model(container,"RF")
#modell.NNET <- train_model(container,"NNET")
#modell.TREE <- train_model(container,"TREE")
```

Nun können wir die vier Modelle anwenden, also die verbleibenden Daten klassifizieren. Solange die Entwicklung eines Klassifikationsmodells noch nicht abgeschlossen ist, wendet man das Modell zumeist auf Daten an, die ebenfalls schon annotiert sind, weil dies es erlaubt, präzise Aussagen über die Genauigkeit des Algorithmus zu machen, indem man die vorgeschlagene Klassfikation mit der Annotation vergleicht. Je höher die Übereinstimmung, desto besser das Modell, mit der Einschränkungen, dass die Übertragbarkeit auf neue, unbekannte Daten i.d.R ein Ziel darstellt, welches schwerer wiegt, als ein perfektes Klassifikationsergebnis für einen bekannten Datensatz. 
```{r Anhand der Modelle klassifizieren}
SVM_CLASSIFY <- classify_model(container, modell.SVM)
GLMNET_CLASSIFY <- classify_model(container, modell.GLMNET)
MAXENT_CLASSIFY <- classify_model(container, modell.MAXENT)
SLDA_CLASSIFY <- classify_model(container, modell.SLDA)
#BOOSTING_CLASSIFY <- classify_model(container, modell.BOOSTING)
#BAGGING_CLASSIFY <- classify_model(container, modell.BAGGING)
#RF_CLASSIFY <- classify_model(container, modell.RF)
#NNET_CLASSIFY <- classify_model(container, modell.NNET)
#TREE_CLASSIFY <- classify_model(container, modell.TREE)
```

Zunächst lassen wir uns zentrale Informationen dazu generieren, wie genau die vier Klassifikatoren unsere Daten annotiert haben. Von RTextTools erhält man hierzu sehr genaue Informationen. 

```{r Performance der unterschiedlichen Algorithmen bestimmen}
analytics <- create_analytics(container, cbind(SVM_CLASSIFY, GLMNET_CLASSIFY, MAXENT_CLASSIFY, SLDA_CLASSIFY))
summary(analytics)
```

Auch hier lässt sich das Ergebnis plotten, um einen Eindruck davon zu erhalten, wie die Algorithmen im direkten Vergleich nach Kategorie abschneiden, und zwar bezüglich der Maße Precision und Recall, also sowohl in Hinblick auf ihre Genauigkeit, als auch mit Blick auf ihre Vollständigkeit. Das folgende Plot zeigt drei Metrien für vier Algorithmen und sechs Inhaltskategorien. 

```{r Performancevergleich von vier Algorithmen für sechs Inhaltskategorien}
topic.codes <- scan("daten/nytimes/majortopics2digits.txt", what = "char", sep = "\n", quiet = T)
topic.codes <- data.frame(category = as.factor(1:length(topic.codes)), category.label = topic.codes)
algdf <- data.frame(algorithm = str_split(colnames(analytics@algorithm_summary), "_", simplify = T)[,1], measure = factor(str_split(colnames(analytics@algorithm_summary), "_", simplify = T)[,2], levels = str_split(colnames(analytics@algorithm_summary), "_", simplify = T)[1:3,2]), category = factor(rep(rownames(analytics@algorithm_summary), each = ncol(analytics@algorithm_summary)), levels = rownames(analytics@algorithm_summary)), score = as.vector(t(analytics@algorithm_summary)))
algdf <- left_join(algdf, topic.codes, by = "category")
algdf <- filter(algdf, category.label %in% c("Civil Rights, Minority Issues, and Civil Liberties", "Education", "Environment", "Law, Crime, and Family Issues", "Macroeconomics", "Sports and Recreation"))
ggplot(algdf, aes(score, algorithm, color = measure, shape = measure)) + geom_point(size = 1.75) + facet_grid(category.label ~ ., switch = "y") + xlim(c(0,1)) + theme(strip.text.y = element_text(angle = 180)) + scale_colour_manual(values = c("lightblue", "lightgreen", "red")) + ggtitle("Genauigkeit, Trefferquote und F-Maß\nvon vier Algorithmen für sechs manuell\nkodierte Inhaltskategorien") + xlab("") + ylab("")

```

### Verwenden von Metadaten als Klassifikation: Ressort in Die Zeit 

Bisher haben wir uns auschließlich mit Daten aus einer tatsächlichen Inhaltsanalyse beschäftigt, aber wie sieht es mit Metadaten aus, welche sich als Kategorisierung einsetzen lassen, ohne dass sie dies im inhaltsanalytischen Sinne tatsächlich sind? Folgend laden wir die bereits in Kapitel 5 verwendeten Daten aus der Wochzeitung *Die Zeit*, arbeiten allerdings diesmal mit den Datensatz, welcher nur die Titel der Beiträge enthält. Wir bestimmen anhand der URL des Artikels das Ressort, filtern nach den frequentesten Ressorts, ziehen ein Zufallssample, und berechnen schließlich eine DFM. 

Anchließend wenden wir zwei Algorithmen aus RTextTools an und vergleichen ihre Performance (mittels einfacher *precision*) mit einem Zufallsalgorithmus. 

```{r Vorhersage von zwei Algorithmen für das Zeit-Korpus anhand des Ressorts}
load("daten/zeit/zeit.korpus.RData")
docvars(korpus.zeit, "ressort") <- str_split(url_parse(korpus.zeit.stats$href)$path, pattern = "/", simplify = T)[,1]
docvars(korpus.zeit, "docID") <- docnames(korpus.zeit)
korpus.zeit.sample <- corpus_subset(korpus.zeit, ressort %in% c("politik", "wirtschaft", "gesellschaft", "sport", "kultur", "wissen", "karriere", "digital"))
zufallssample.ressorts <- data.frame(doc_id = docnames(korpus.zeit.sample), docvars(korpus.zeit.sample), stringsAsFactors = F) %>% group_by(ressort) %>% sample_n(1000)
korpus.zeit.sample <- corpus_subset(korpus.zeit.sample, docID %in% zufallssample.ressorts$doc_id)
meine.dfm.trim <- dfm_trim(dfm(korpus.zeit.sample), min_docfreq = 2)
container <- create_container(convert(meine.dfm.trim, to = "matrix"), docvars(korpus.zeit.sample)$ressort, trainSize = 1:7200, testSize = 7201:8000, virgin = FALSE)
modelle.MAXENT.SVM.TREE <- train_models(container, algorithms = c("MAXENT", "SVM"))
klassifikation.MAXENT.SVM <- classify_models(container, modelle.MAXENT.SVM.TREE)
prop.table(table(sample(docvars(korpus.zeit.sample)$ressort) == docvars(korpus.zeit.sample)$ressort))*100 # Zufall
prop.table(table(klassifikation.MAXENT.SVM$MAXENTROPY_LABEL == docvars(korpus.zeit.sample)$ressort))*100 # Maximum Entropy
prop.table(table(klassifikation.MAXENT.SVM$SVM_LABEL == docvars(korpus.zeit.sample)$ressort))*100 # SVM
```

Dieses Ergebnis ist zunächst einmal alles andere als berauschend. Sowohl der Zufallsalgorithmus, als auch Maximum Entropy und SVM, schneiden ausgesprochen schlecht ab, wobei der Vergleich mit dem Zufall verdeutlicht, dass nur cica 2-3% der Schlagzeilen auf Grundlage des jeweiligen Modells erkannt werden. Allerdings muss man berücksichtigen dass es (a) um die Beziehung von Text und Ressort geht (die auch verschiedenen Gründen nicht sehr eng sein muss) und (b) dass lediglich der Titel des jeweiligen Artikels die Grundlage der Klassifikation bildet, und nicht etwa der Volltext. Wie unterscheidet sich die Qualität der Klassifikation nach Ressort?

```{r Korrekt klassifizierte Texte nach Ressort}
ergebnis.MAXENT<- data.frame(MAXENT = as.character(klassifikation.MAXENT.SVM$MAXENTROPY_LABEL), Kategorie = docvars(korpus.zeit.sample)$ressort, stringsAsFactors = F) %>% 
	mutate(Algorithmus = "MAXENT") %>% 
	mutate(TF = MAXENT == Kategorie) %>% 
	group_by(Algorithmus, Kategorie, TF) %>% 
	summarise(n = n()) %>% 
	mutate(Anteil = n/sum(n)) %>% 
  filter(TF == TRUE)
ergebnis.SVM <- data.frame(SVM = as.character(klassifikation.MAXENT.SVM$SVM_LABEL), Kategorie = docvars(korpus.zeit.sample)$ressort, stringsAsFactors = F) %>% 
	mutate(Algorithmus = "SVM") %>% 
	mutate(TF = SVM == Kategorie) %>% 
	group_by(Algorithmus, Kategorie, TF) %>% 
	summarise(n = n()) %>% 
	mutate(Anteil = n/sum(n)) %>% 
  filter(TF == TRUE)
ergebnis.RANDOM <- data.frame(RANDOM = sample(docvars(korpus.zeit.sample)$ressort), Kategorie = docvars(korpus.zeit.sample)$ressort, stringsAsFactors = F) %>% 
	mutate(Algorithmus = "RANDOM") %>% 
	mutate(TF = RANDOM == Kategorie) %>% 
	group_by(Algorithmus, Kategorie, TF) %>% 
	summarise(n = n()) %>% 
	mutate(Anteil = n/sum(n)) %>% 
  filter(TF == TRUE)
ergebnis.MAXENT.SVM <- bind_rows(ergebnis.MAXENT, ergebnis.SVM, ergebnis.RANDOM)
ggplot(ergebnis.MAXENT.SVM, aes(Kategorie, Anteil, colour = Algorithmus, group = Algorithmus)) + geom_line(size = 1) + ylim(c(0,1)) + scale_colour_brewer(palette = "Set1") + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + ggtitle("Korrekt klassifizierte Zeit-Artikel mit zwei Algorithmen nach Ressort") + xlab("") + ylab("korrekt klassifiziert")
```

Es fällt auf, dass Maximum Entropy sich in diesem Fall nicht signifikant vom Zufallsalgorithmus unterscheidet, während SVM immerhin etwas besser abschneidet. Das Ergebnis für das Poltik-Ressort scheint hier auf den ersten Blick für SVM deutlich besser zu sein, als für die verbleibenden Kategorien, was allerdings auch an der sehr ungleichen Verteilung der Ressorts im Gesamtkorpus liegt (SVM überschätzt den Anteil der Beiträge aus dem Bereicht Politik schlicht). Trotzdem sollte das Fazit deshalb nicht zu ernüchtert ausfallen, weil klar wird, dass selbst die Beitragstitel in einem relativ kleinen Sample ein gewisses Vorhersagepotenzial für das Ressort haben, was ja auch zu erwarten war. Man kann sich so leicht vorstellen, in bestimmten Fällen mit der richtigen Kombination von Metadaten zu einem guten Klassifikationsergebnis zu kommen, und das ganz ohne zuvor eine manuellen Inhaltsanalyse durchgeführt zu haben. 
